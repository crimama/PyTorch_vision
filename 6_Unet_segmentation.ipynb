{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet을 이용한 Image Segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레퍼런스 : https://colab.research.google.com/github/dhrim/MDC_2021/blob/master/material/deep_learning/unet_segmentation_multi_label.ipynb#scrollTo=cHnifLferK9Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로우에서 데이터 받아올 때 gpu 사용하는 듯 \n",
    " 수정 필요 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds \n",
    "import numpy as np \n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt \n",
    "#데이터 로드 \n",
    "\n",
    "def image_data_load():\n",
    "    ds =tfds.load('lost_and_found',split='train',batch_size= 200 )\n",
    "    dataset = next(iter(ds))\n",
    "    images = dataset['image_left'].numpy()\n",
    "    labels = dataset['segmentation_label'].numpy()\n",
    "    return images ,labels \n",
    "\n",
    "class divide(nn.Module):\n",
    "    def __init__(self,divide_value):\n",
    "        super().__init__()\n",
    "        self.divide_value = divide_value\n",
    "\n",
    "    def forward(self,img):\n",
    "        return img/self.divide_value\n",
    "\n",
    "def image_transform():\n",
    "    transform = transforms.Compose([\n",
    "        divide(255),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((256,256)),\n",
    "        transforms.Normalize(mean=0.5,std=0.5)\n",
    "    ])\n",
    "    return transform \n",
    "\n",
    "class Dset(Dataset):\n",
    "    def __init__(self,images,labels,transform):\n",
    "        super().__init__()\n",
    "        self.images = images \n",
    "        self.labels = labels \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def image_transform(self,img):\n",
    "        return self.transform(img)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        image = self.image_transform(self.images[idx])\n",
    "        label = self.image_transform(self.labels[idx])\n",
    "\n",
    "        return image, label \n",
    "\n",
    "def train_valid_split(images,labels):\n",
    "    length = len(images)\n",
    "    split_index = int(length*0.8)\n",
    "    train_images,test_images = images[:split_index], images[split_index:]\n",
    "    train_labels,test_labels = labels[:split_index], labels[split_index:]\n",
    "    return train_images,test_images,train_labels,test_labels \n",
    "\n",
    "class Conv_Block(nn.Module):\n",
    "    def __init__(self,input_c,output_c):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(in_channels=input_c,out_channels=output_c,kernel_size=3,padding=1)\n",
    "        self.conv = nn.Conv2d(in_channels=output_c,out_channels=output_c,kernel_size=3,padding=1)\n",
    "        self.batchnorm = nn.BatchNorm2d(output_c)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv_in(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.conv(x)\n",
    "        x = F.relu(x)\n",
    "        conv = self.batchnorm(x)\n",
    "        pool = self.maxpool(conv)\n",
    "        return conv,pool\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv_Block(3,32)\n",
    "        self.conv2 = Conv_Block(32,64)\n",
    "        self.conv3 = Conv_Block(64,128)\n",
    "        self.conv4 = Conv_Block(128,256)\n",
    "        self.conv5 = self.Conv_last(256,512)\n",
    "\n",
    "    def Conv_last(self,input_c,output_c):\n",
    "        block = nn.Sequential(\n",
    "                                nn.Conv2d(input_c,output_c,3,padding=1),\n",
    "                                nn.ReLU(),\n",
    "                                nn.BatchNorm2d(output_c),\n",
    "                                nn.Conv2d(output_c,output_c,3,padding=1),\n",
    "                                nn.ReLU(),\n",
    "                                nn.BatchNorm2d(output_c)\n",
    "        )\n",
    "        return block \n",
    "\n",
    "    def forward(self,x):\n",
    "        conv1,pool1 = self.conv1(x)\n",
    "        conv2,pool2 = self.conv2(pool1)\n",
    "        conv3,pool3 = self.conv3(pool2)\n",
    "        conv4,pool4 = self.conv4(pool3)\n",
    "        conv5 = self.conv5(pool4)\n",
    "        return conv1,conv2,conv3,conv4,conv5\n",
    "\n",
    "class Conv_up_block(nn.Module):\n",
    "    def __init__(self,input_c,output_c):\n",
    "        super().__init__()\n",
    "        self.up_sample = nn.ConvTranspose2d(input_c,output_c,2,stride=2,padding=0)\n",
    "        self.conv1 = nn.Conv2d(input_c,output_c,3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(output_c,output_c,3,padding=1)\n",
    "        self.batchnorm = nn.BatchNorm2d(output_c)\n",
    "\n",
    "\n",
    "    def forward(self,x,conv):\n",
    "        self.up = self.up_sample(x)\n",
    "        x = torch.cat((self.up,conv),dim=1)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchnorm(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchnorm(x)\n",
    "        \n",
    "        return x \n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_up1 = Conv_up_block(512,256)\n",
    "        self.conv_up2 = Conv_up_block(256,128)\n",
    "        self.conv_up3 = Conv_up_block(128,64)\n",
    "        self.conv_up4 = Conv_up_block(64,32)\n",
    "        self.conv_last = nn.Conv2d(32,1,1,padding=0)\n",
    "    \n",
    "    def forward(self,conv1,conv2,conv3,conv4,conv5):\n",
    "        up = self.conv_up1(conv5,conv4)\n",
    "        up = self.conv_up2(up,conv3)\n",
    "        up = self.conv_up3(up,conv2)\n",
    "        up = self.conv_up4(up,conv1)\n",
    "        up = self.conv_last(up)\n",
    "        up = F.softmax(up,dim=1)\n",
    "        return up \n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Down = Down()\n",
    "        self.Up = Up()\n",
    "\n",
    "    def forward(self,x):\n",
    "        conv1,conv2,conv3,conv4,conv5 = self.Down(x)\n",
    "        x = self.Up(conv1,conv2,conv3,conv4,conv5)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-11 06:35:10.188095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-11 06:35:10.196122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-11 06:35:10.196658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-11 06:35:10.197937: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-11 06:35:10.198359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-11 06:35:10.198867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-11 06:35:10.199364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-11 06:35:10.595755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-11 06:35:10.596110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-11 06:35:10.596417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-11 06:35:10.596706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9410 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    batch_size = 2\n",
    "    epoch = 50\n",
    "\n",
    "images,labels = image_data_load() \n",
    "train_images,test_images,train_labels,test_labels = train_valid_split(images,labels)\n",
    "\n",
    "image_transformer = image_transform()\n",
    "train_dataset = Dset(train_images, train_labels,image_transformer)\n",
    "test_dataset = Dset(test_images,test_labels,image_transformer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,CFG.batch_size,shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset,CFG.batch_size,shuffle=False)\n",
    "\n",
    "unet = Unet()\n",
    "a,b = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.to('cuda')\n",
    "# a = a.to('cuda').type(torch.float)\n",
    "# a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 11 06:35:26 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 31%   40C    P2    63W / 250W |  10765MiB / 11018MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
